# from cybercomp import Experiment

# if __name__ == "__main__":
#     # list all models

#     # i pick model Z

#     # define four models
#     model_A = Model(name="", config=dict())
#     model_B = Model(name="", config=dict())
#     model_C = Model(name="", config=dict())

#     Model, Engine, (params, params)


#     listX = TypeChecker.find_matches("modelA", "engineB")

#     [
#         "progA", "script_for_config_"
#     ]

#     config = listX[0]

#     print()

#     Model(name="")

#     engine_A = Engine(name="euler", config=dict())
#     engine_B = Engine(name="rk4", config=dict())
#     engine_C = Engine(name="dopri5", config=dict())

#     comp = Computation(
#         model=model_A,
#         engine=engine_A,
#         config=ComputationConfig[dict, dict, dict](
#             inputs=dict(),
#             observations=dict(),
#             hyperparams=dict(),
#         ),
#     )
#     comp.validate()
#     comp.execute(config)


#     collectionA = []

#     for i in range(10,1000):
#         for j in range(20,100):
#             experiment = comp.execute(config, params=(i,j))
#             collectionA.append(experiment)

#     collectionB = []
#     for i in range(100,100):
#         for j in range(200,1000):
#             experiment = comp.execute(config, params=(i,j))
#             collectionB.append(experiment)


#     collect(handles)


#     # ex 1 - same thing run with randomness (n trials)
#     # ex 2 - change a parameter, keeping everything else fixed (m values for params)

#     fetch(collectionA)

#     compare(collectionA, collectionB) # should be comparable, if they have **similar observations**

#     post_analysis

    





# # search interface for models, parameters, 


# model = [False]
# params = [None]



# [Validate] OK

# [Run]


# #


# model A: "network" -> "{NETWORK_CONFIG}"
# model B: "simulation" {NETWORK_CONFIG}]
# model ] ["network" -> "simulation"]

# expD = (expA->expB) -> expC
# expE = (expB) -> expC


# compare(expD, expE)
